{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the Data Using Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the dataset has a unique ID. In the following code, when defining the generator, we first get the list of ID's from the RGB folder, then use the list of ID's to load the corresponding information of each sample. Associated with each sample is the following information (size and directory name for the training set are also listed):\n",
    "    1. RGB image (512 * 512 * 3) (train/images/rgb)\n",
    "    2. NIR image (512 * 512 * 1) (train/images/rgb)\n",
    "    3. Boundary (512 * 512 * 1) (train/boundaries)\n",
    "    4. Mask (512 * 512 * 1) (train/masks)\n",
    "The task of this project is to use the information above to predict for each pixel, which of the six categories it belons to:\n",
    "    1. Cloud shadow (512 * 512 * 1) (train/labels/cloud_shadow)\n",
    "    2. Double plant (512 * 512 * 1) (train/labels/double_plant)\n",
    "    3. Planter skip (512 * 512 * 1) (train/labels/planter_skip)\n",
    "    4. Standing water (512 * 512 * 1) (train/labels/standing_water)\n",
    "    5. Waterway (512 * 512 * 1) (train/labels/waterway)\n",
    "    6. Weed cluster (512 * 512 * 1) (train/labels/weed_cluster)\n",
    "If a pixel does not belong to any category above, it is considered to be\n",
    "    0. Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import Callable, Union\n",
    "from itertools import product\n",
    "from scipy import stats\n",
    "\n",
    "#keras imports\n",
    "import keras\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input, BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.convolutional import Conv2DTranspose\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "%env SM_FRAMEWORK=tf.keras\n",
    "import albumentations as A\n",
    "import segmentation_models as sm \n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_check(path, ntail, ids):\n",
    "    fignames = os.listdir(path)\n",
    "    for fig_id in ids:\n",
    "        if not fig_id + ntail in fignames:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def make_label(raw_labels):\n",
    "    \"\"\"\n",
    "    This function creates a 7-channel label from the six labels (add a background channel).\n",
    "    \"\"\"\n",
    "    stacked_labels = np.stack(raw_labels, axis=-1)\n",
    "    last_label = np.sum(stacked_labels, axis=-1) + 1\n",
    "    last_label[last_label > 1] = 0\n",
    "    # Put the background as the last channel of the label\n",
    "    full_labels =  np.concatenate([stacked_labels, last_label.reshape((512, 512, 1))], axis=-1)\n",
    "    return full_labels / np.linalg.norm(full_labels, axis=-1).reshape((512, 512, 1))\n",
    "    \n",
    "def visualize(**images):\n",
    "    \"\"\"PLot images in one row.\"\"\"\n",
    "    n = len(images)\n",
    "    plt.figure(figsize=(16, 5))\n",
    "    for i, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n, i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(' '.join(name.split('_')).title())\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "def img_gen(dataset='train'):\n",
    "    rgb_path = os.path.join(dataset, 'images', 'rgb')\n",
    "    nir_path = os.path.join(dataset, 'images', 'nir')\n",
    "    bdry_path = os.path.join(dataset, 'boundaries')\n",
    "    mask_path = os.path.join(dataset, 'masks')\n",
    "    label_names = ['cloud_shadow', 'double_plant', 'planter_skip', 'standing_water', 'waterway', 'weed_cluster']\n",
    "    label_paths = [os.path.join(dataset, 'labels', label_name) for label_name in label_names]\n",
    "    label = np.zeros((512,512,7))\n",
    "    \n",
    "    rgb_fig_names = os.listdir(rgb_path)\n",
    "    fig_ids = [fname[:-4] for fname in rgb_fig_names]\n",
    "    for fig_id in fig_ids:\n",
    "        rgb_img = mpimg.imread(os.path.join(rgb_path, fig_id + '.jpg'))\n",
    "        nir_img = mpimg.imread(os.path.join(nir_path, fig_id + '.jpg')).reshape((512, 512, 1))\n",
    "        bdry_img = mpimg.imread(os.path.join(bdry_path, fig_id + '.png'))\n",
    "        mask_img = mpimg.imread(os.path.join(mask_path, fig_id + '.png'))\n",
    "        if dataset != \"test\":\n",
    "            label_imgs = [mpimg.imread(os.path.join(label_path, fig_id + '.png')) for label_path in label_paths]\n",
    "            label = make_label(label_imgs)\n",
    "            \n",
    "        input_img = np.concatenate([rgb_img, nir_img], axis=2) / 255. # Concatenate the RGB and NIR\n",
    "        \n",
    "        yield fig_id, input_img, bdry_img, mask_img, label\n",
    "    \n",
    "   \n",
    "    \n",
    "def masked_img_gen(dataset='train', use_augmentation=False):\n",
    "    aug = A.Compose([  \n",
    "        A.VerticalFlip(p=0.5),              \n",
    "        A.RandomRotate90(p=0.5),]) \n",
    "    \n",
    "    for fig_id, input_img, bdry_img, mask_img, label in img_gen(dataset):\n",
    "\n",
    "        \n",
    "        final_mask = np.multiply(bdry_img, mask_img).reshape(512, 512, 1) # Form the final mask\n",
    "        masked_img = np.multiply(final_mask, input_img)\n",
    "        masked_img = masked_img.reshape(-1, 512, 512, 4)\n",
    "        \n",
    "        if use_augmentation:\n",
    "            augmented = aug(image=masked_img, mask=label)\n",
    "            masked_img = augmented['image']\n",
    "            label = augmented['mask']\n",
    "                    \n",
    "        if dataset != 'test':\n",
    "            label = label.reshape(-1, 512, 512, 7)\n",
    "            yield masked_img, label\n",
    "\n",
    "        else:\n",
    "            yield masked_img, fig_id\n",
    "            \n",
    "def rgb_masked_img_gen(dataset='train', use_augmentation=False):\n",
    "    if dataset != 'test':\n",
    "        for masked_img, label in masked_img_gen(dataset, use_augmentation):\n",
    "            yield masked_img[:,:,:,0:3], label\n",
    "        \n",
    "    else:\n",
    "        for masked_img, fig_id in masked_img_gen(dataset, use_augmentation):\n",
    "             yield masked_img[:,:,:,0:3], fig_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "               kernel_initializer = 'he_normal', padding = 'same', activation = 'relu')(input_tensor)\n",
    "    \n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same', activation = 'relu')(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def createUnet(input_img, n_filters = 16, dropout = 0.1):\n",
    "    #Down the Unet\n",
    "    conv1 = conv2d_block(input_img, n_filters)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(dropout)(pool1)\n",
    "    \n",
    "    conv2 = conv2d_block(pool1, n_filters*2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(dropout)(pool2)\n",
    "    \n",
    "    conv3 = conv2d_block(pool2, n_filters*4)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(dropout)(pool3)\n",
    "    \n",
    "    conv4 = conv2d_block(pool3, n_filters*8)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout)(pool4)\n",
    "    #bottom of the unet\n",
    "    conv5 = conv2d_block(pool4, n_filters*16)\n",
    "    \n",
    "    #up the U\n",
    "    uconv4 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(conv5)\n",
    "    uconv4 = concatenate([uconv4, conv4])\n",
    "    uconv4 = conv2d_block(uconv4, n_filters*8)\n",
    "    \n",
    "    uconv3 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(uconv4)\n",
    "    uconv3 = concatenate([uconv3, conv3])\n",
    "    uconv3 = conv2d_block(uconv3, n_filters*4)\n",
    "    \n",
    "    uconv2 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(uconv3)\n",
    "    uconv2 = concatenate([uconv2, conv2])\n",
    "    uconv2 = conv2d_block(uconv2, n_filters*2)\n",
    "    \n",
    "    uconv1 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(uconv2)\n",
    "    uconv1 = concatenate([uconv1, conv1])\n",
    "    uconv1= conv2d_block(uconv1, n_filters*1)\n",
    "    \n",
    "    output = Conv2D(filters = 7, kernel_size = (1,1), activation = \"softmax\", padding = 'same')(uconv1)\n",
    "    model = Model(inputs=[input_img], outputs=[output])\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "\n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "\n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    weights = K.variable(weights)\n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "        \n",
    "    return loss\n",
    "\n",
    "def multiclass_weighted_dice_loss(class_weights: Union[list, np.ndarray, tf.Tensor]) -> Callable[[tf.Tensor, tf.Tensor], tf.Tensor]:\n",
    "    \"\"\"\n",
    "    Weighted Dice loss.\n",
    "    Used as loss function for multi-class image segmentation with one-hot encoded masks.\n",
    "    :param class_weights: Class weight coefficients (Union[list, np.ndarray, tf.Tensor], len=<N_CLASSES>)\n",
    "    :return: Weighted Dice loss function (Callable[[tf.Tensor, tf.Tensor], tf.Tensor])\n",
    "    \"\"\"\n",
    "    if not isinstance(class_weights, tf.Tensor):\n",
    "        class_weights = tf.constant(class_weights)\n",
    "\n",
    "    def loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Compute weighted Dice loss.\n",
    "        :param y_true: True masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, <N_CLASSES>))\n",
    "        :param y_pred: Predicted masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, <N_CLASSES>))\n",
    "        :return: Weighted Dice loss (tf.Tensor, shape=(None,))\n",
    "        \"\"\"\n",
    "        axis_to_reduce = range(1, K.ndim(y_pred))  # Reduce all axis but first (batch)\n",
    "        numerator = y_true * y_pred * class_weights  # Broadcasting\n",
    "        numerator = 2. * K.sum(numerator, axis=axis_to_reduce)\n",
    "\n",
    "        denominator = (y_true + y_pred) * class_weights # Broadcasting\n",
    "        denominator = K.sum(denominator, axis=axis_to_reduce)\n",
    "\n",
    "        return 1 - numerator / denominator\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_reduction=ReduceLROnPlateau(monitor=\"MeanIoU\", patience=2, verbose=0, factor=0.5, min_lr=0.0001)\n",
    "\n",
    "train_path = os.path.join('train', 'images', 'rgb')\n",
    "val_path = os.path.join('val', 'images', 'rgb')\n",
    "\n",
    "steps_per_epoch = len(os.listdir(train_path))//32\n",
    "validation_steps = len(os.listdir(val_path))//32\n",
    "\n",
    "print(\"Steps per epoch: {}\".format(steps_per_epoch))\n",
    "print(\"Validation steps: {}\".format(validation_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "input_img = Input((512, 512, 4), name='img')\n",
    "model = createUnet(input_img, n_filters = 32, dropout = 0.2)\n",
    "\n",
    "weights = 5 * np.ones((7,), dtype=np.float32)\n",
    "weights[-1] = .1\n",
    "print(weights)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=multiclass_weighted_dice_loss(weights), metrics = [tf.keras.metrics.MeanIoU(num_classes=7, name = \"MeanIoU\")])\n",
    "# model.compile(optimizer=\"adam\", loss=weighted_categorical_crossentropy(weights), metrics = [tf.keras.metrics.MeanIoU(num_classes=7, name = \"MeanIoU\")])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 30\n",
    "model.fit(masked_img_gen('train', False), epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=masked_img_gen('val'), validation_steps=validation_steps, use_multiprocessing=False, shuffle=True,\\\n",
    "         callbacks = [learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKBONE = 'resnet34'\n",
    "preprocess_input = sm.get_preprocessing(BACKBONE)\n",
    "\n",
    "# load your data\n",
    "# x_train, y_train, x_val, y_val = load_data(...)\n",
    "\n",
    "# # preprocess input\n",
    "# x_train = preprocess_input(x_train)\n",
    "# x_val = preprocess_input(x_val)\n",
    "\n",
    "weights = 2 * np.ones((7,))\n",
    "weights[-1] = 0.5\n",
    "\n",
    "# define model\n",
    "model = sm.Unet(BACKBONE, encoder_weights='imagenet', input_shape = (512,512,3), classes = 7, activation = 'softmax')\n",
    "model.compile(\n",
    "    'Adam',\n",
    "    loss=sm.losses.DiceLoss(class_weights = weights),\n",
    "    metrics=[sm.metrics.IOUScore(class_weights = weights, name= \"MeanIoU\")],\n",
    ")\n",
    "\n",
    "# fit model\n",
    "# if you use data generator use model.fit_generator(...) instead of model.fit(...)\n",
    "# more about `fit_generator` here: https://keras.io/models/sequential/#fit_generator\n",
    "# model.fit(,\n",
    "#    batch_size=16,\n",
    "#    epochs=100,\n",
    "#    validation_data=(x_val, y_val),\n",
    "# )\n",
    "epochs = 40\n",
    "tf.keras.backend.clear_session()\n",
    "# model.summary()\n",
    "model.fit(rgb_masked_img_gen('train'), epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=rgb_masked_img_gen('val'), validation_steps=validation_steps, use_multiprocessing=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def outputTestImages(model, gen, ensemble):\n",
    "    if not os.path.isdir('preds'):\n",
    "        os.mkdir('preds')\n",
    "\n",
    "    for mask_img, fig_id in gen('test'):\n",
    "        y_pred = None\n",
    "        if ensemble:\n",
    "            y_pred = model(mask_img, 0)\n",
    "        else:\n",
    "            y_pred= model.predict(mask_img, verbose=0)\n",
    "\n",
    "        pred_result = y_pred[0,:,:,:] # take the ith predictio\n",
    "\n",
    "\n",
    "        # Since we appended the background to the last channel, we need to bring it to the front when saving predictions\n",
    "        processed_result = np.concatenate([pred_result[:, :, -1].reshape((512, 512, 1)), pred_result[:, :, :-1]], axis=-1)\n",
    "    #     print(processed_result.shape) # Should be (512, 512, 7)\n",
    "        # Convert the 7-channel result to 1-channel result and cast to uint8\n",
    "\n",
    "        final_pred = np.argmax(processed_result, axis=-1).astype(np.uint8)\n",
    "    #     print(final_pred.shape) # Should be (512, 512)\n",
    "        # Save the prediction\n",
    "        filename = os.path.join('preds', fig_id + '.png')\n",
    "        Image.fromarray(final_pred).save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "            # rename to avoid 'unique layer name' issue\n",
    "            layer._name = 'ensemble_' + str(i+1) + '_' + layer.name\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = output = Conv2D(filters = 3, kernel_size = (1,1), activation = \"relu\", padding = 'same')(merge)\n",
    "    output = output = Conv2D(filters = 7, kernel_size = (1,1), activation = \"softmax\", padding = 'same')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "    # plot graph of ensemble\n",
    "#     plot_model(model, show_shapes=True, to_file='model_graph.png')\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def buildAndTrainEnsemble(numUnet, lossFunc, optimizer, epochs):\n",
    "    keras.backend.clear_session()\n",
    "    models = []\n",
    "    input_img = Input((512, 512, 4), name='img')\n",
    "    \n",
    "    for i in range (numUnet):\n",
    "        \n",
    "        \n",
    "        model = createUnet(input_img)\n",
    "        \n",
    "        # weights = np.ones((7,7))\n",
    "        # weights[:,6] = 1.3\n",
    "        # weights[6,:] = 1.3\n",
    "        model.compile(optimizer=optimizer, loss=lossFunc, metrics = [tf.keras.metrics.MeanIoU(num_classes=7)])\n",
    "        models.append(model)\n",
    "    for m in models:\n",
    "        m.fit(masked_img_gen('train'), epochs=epochs, steps_per_epoch=403, validation_data=masked_img_gen('val'), validation_steps=138, use_multiprocessing=False, shuffle=True)\n",
    "    return models\n",
    "    \n",
    "def predictEnsemble(mods):\n",
    "    def predict(img, verbose):\n",
    "        preds = None\n",
    "        for m in mods:\n",
    "            pred = m.predict(img, verbose = verbose)\n",
    "#             print (pred[0,0,0])\n",
    "#             for i in range(7):\n",
    "#                 pred[:, :, :,i] *= i+1\n",
    "#             pred = np.sum(pred, axis= 3)\n",
    "#             print (pred[0,0,0])\n",
    "            if preds is None:\n",
    "                preds = pred\n",
    "            else:\n",
    "#                 np.stack([preds, pred], axis = -1)\n",
    "#                 np.dstack((preds, pred))\n",
    "                preds += pred\n",
    "#         print(preds.shape)\n",
    "#         print(to_categorical(preds, 7)[0,0,0])\n",
    "#         results = stats.mode(preds, axis = 3)\n",
    "#         preds = preds.reshape((preds.shape[0], preds.shape[1]*preds.shape[2]))\n",
    "        return preds\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamOpt = keras.optimizers.Adam(lr = .0001)\n",
    "weights = 10 * np.ones((7,), dtype=np.float32)\n",
    "weights[0] = .3\n",
    "lossFunc = multiclass_weighted_dice_loss(weights)\n",
    "\n",
    "models = buildAndTrainEnsemble(3, lossFunc, adamOpt, 20)\n",
    "\n",
    "# print(adamOpt.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_gen = masked_img_gen('val', False)\n",
    "new_img_gen = img_gen('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_img, label = next(new_gen)\n",
    "fig_id, input_img, bdry_img, mask_img_old, label_old = next(new_img_gen)\n",
    "visualize(\n",
    "    original=input_img[:,:,0:3],\n",
    "    original_nir = input_img[:,:,3],\n",
    "    masked_image=mask_img[0,:,:,0:3], \n",
    "    masked_nir=mask_img[0,:,:,3],\n",
    "#     idx_0=label[0,:,:,0],\n",
    "#     idx_1=label[0,:,:,1],\n",
    "#     idx_2=label[0,:,:,2],\n",
    "#     idx_3=label[0,:,:,3],\n",
    "    idx_4=label[0,:,:,4],\n",
    "#     idx_5=label[0,:,:,5],\n",
    "    background=label[0,:,:,6]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred= predictEnsemble(models)(mask_img, verbose=0)\n",
    "\n",
    "pred_result = y_pred[0,:,:,:] # take the ith predictio\n",
    "\n",
    "processed_result = np.concatenate([pred_result[:, :, -1].reshape((512, 512, 1)), pred_result[:, :, :-1]], axis=-1)\n",
    "\n",
    "final_pred = np.argmax(processed_result, axis=-1).astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred= model.predict(mask_img, verbose=0)\n",
    "\n",
    "pred_result = y_pred[0,:,:,:] # take the ith predictio\n",
    "\n",
    "processed_result = np.concatenate([pred_result[:, :, -1].reshape((512, 512, 1)), pred_result[:, :, :-1]], axis=-1)\n",
    "\n",
    "print(processed_result.shape)\n",
    "\n",
    "final_pred = np.argmax(processed_result, axis=-1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(bg=processed_result[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_label = np.argmax(label[0,:,:,:], axis=-1).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label[0,0,0,:])\n",
    "print(processed_result[0,0,:])\n",
    "print(np.argmax(processed_result[0,0,:]))\n",
    "a = tf.one_hot(tf.argmax(processed_result[0,0,:]), depth = 7)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff = next(img_gen('val'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputTestImages(predictEnsemble(models), masked_img_gen, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputTestImages(model, masked_img_gen, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 12901//32\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_steps = 4431//32\n",
    "print(validation_steps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

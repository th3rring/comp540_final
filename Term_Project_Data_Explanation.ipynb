{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in the Data Using Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample in the dataset has a unique ID. In the following code, when defining the generator, we first get the list of ID's from the RGB folder, then use the list of ID's to load the corresponding information of each sample. Associated with each sample is the following information (size and directory name for the training set are also listed):\n",
    "    1. RGB image (512 * 512 * 3) (train/images/rgb)\n",
    "    2. NIR image (512 * 512 * 1) (train/images/rgb)\n",
    "    3. Boundary (512 * 512 * 1) (train/boundaries)\n",
    "    4. Mask (512 * 512 * 1) (train/masks)\n",
    "The task of this project is to use the information above to predict for each pixel, which of the six categories it belons to:\n",
    "    1. Cloud shadow (512 * 512 * 1) (train/labels/cloud_shadow)\n",
    "    2. Double plant (512 * 512 * 1) (train/labels/double_plant)\n",
    "    3. Planter skip (512 * 512 * 1) (train/labels/planter_skip)\n",
    "    4. Standing water (512 * 512 * 1) (train/labels/standing_water)\n",
    "    5. Waterway (512 * 512 * 1) (train/labels/waterway)\n",
    "    6. Weed cluster (512 * 512 * 1) (train/labels/weed_cluster)\n",
    "If a pixel does not belong to any category above, it is considered to be\n",
    "    0. Background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import image as mpimg\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#keras imports\n",
    "import keras\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.merge import concatenate, add\n",
    "from keras.layers.convolutional import Conv2DTranspose\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Model\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "def id_check(path, ntail, ids):\n",
    "    fignames = os.listdir(path)\n",
    "    for fig_id in ids:\n",
    "        if not fig_id + ntail in fignames:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def make_label(raw_labels):\n",
    "    \"\"\"\n",
    "    This function creates a 7-channel label from the six labels (add a background channel).\n",
    "    \"\"\"\n",
    "    stacked_labels = np.stack(raw_labels, axis=-1)\n",
    "    last_label = np.sum(stacked_labels, axis=-1) + 1\n",
    "    last_label[last_label > 1] = 0\n",
    "    # Put the background as the last channel of the label\n",
    "    full_labels =  np.concatenate([stacked_labels, last_label.reshape((512, 512, 1))], axis=-1)\n",
    "    return full_labels / np.linalg.norm(full_labels, axis=-1).reshape((512, 512, 1))\n",
    "    \n",
    "\n",
    "\n",
    "def img_gen(dataset='train'):\n",
    "    rgb_path = os.path.join(dataset, 'images', 'rgb')\n",
    "    nir_path = os.path.join(dataset, 'images', 'nir')\n",
    "    bdry_path = os.path.join(dataset, 'boundaries')\n",
    "    mask_path = os.path.join(dataset, 'masks')\n",
    "    label_names = ['cloud_shadow', 'double_plant', 'planter_skip', 'standing_water', 'waterway', 'weed_cluster']\n",
    "    label_paths = [os.path.join(dataset, 'labels', label_name) for label_name in label_names]\n",
    "    label = np.zeros((512,512,7))\n",
    "    \n",
    "    rgb_fig_names = os.listdir(rgb_path)\n",
    "    fig_ids = [fname[:-4] for fname in rgb_fig_names]\n",
    "    for fig_id in fig_ids:\n",
    "        rgb_img = mpimg.imread(os.path.join(rgb_path, fig_id + '.jpg'))\n",
    "        nir_img = mpimg.imread(os.path.join(nir_path, fig_id + '.jpg')).reshape((512, 512, 1))\n",
    "        bdry_img = mpimg.imread(os.path.join(bdry_path, fig_id + '.png'))\n",
    "        mask_img = mpimg.imread(os.path.join(mask_path, fig_id + '.png'))\n",
    "        if dataset != \"test\":\n",
    "            label_imgs = [mpimg.imread(os.path.join(label_path, fig_id + '.png')) for label_path in label_paths]\n",
    "            label = make_label(label_imgs)\n",
    "            \n",
    "        input_img = np.concatenate([rgb_img, nir_img], axis=2) / 255. # Concatenate the RGB and NIR\n",
    "        \n",
    "        yield fig_id, input_img, bdry_img, mask_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of positives for each class\n",
    "counts = [0]*6\n",
    "label_names = ['cloud_shadow', 'double_plant', 'planter_skip', 'standing_water', 'waterway', 'weed_cluster']\n",
    "cnt =0\n",
    "for fig_id, input_img, bdry_img, mask_img, label in img_gen():\n",
    "    cnt += 1\n",
    "\n",
    "    for i in range(len(label_names)):\n",
    "        counts[i]+= np.sum(label[:,:,i])\n",
    "        # print(label[:,:,i], np.sum(label[:,:,i]))\n",
    "    if cnt == 5000:\n",
    "        break\n",
    "\n",
    "for i in range(len(label_names)):\n",
    "    print(label_names[i],counts[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(len(label_names)), counts)\n",
    "plt.xticks(range(len(label_names)), label_names, rotation = 45)\n",
    "plt.title(\"Number of pixels with a certain \\n class in the first 5000 images\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize a Training Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we see a visualization of the training sample. The first four images are RGB image, NIR image, boundary and mask. Multiplying the boundary with the mask gives you the \"final mask\", in which the pixels that you need to classify are marked white (with value 1), and the pixels you don't need to classify are marked black (with value 0). The next three images shows the RGB and the NIR image after you apply the boundary and the mask, and the \"final mask\".\n",
    "\n",
    "Continuing to the next six images, these shows the labels corresponding to cloud shadow, double plant, planter skip, standing water, water way, and weed cluster. Pixels marked white (with value 1) in the image belongs to the category of the corresponding image. For instance, in the last label image (corresponding to the water way), the pixels that appears white belongs to the category of waterway.\n",
    "\n",
    "The last image shows the background. A pixel belongs to the category of background if it does not belong to any of the six categories above.\n",
    "\n",
    "You can change the \"sample_idx\" to see more examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pay attention to the \"make_label\" function in the first code block. After reading in the label of a sample, you will get six arrays each with size (512 * 512). However, it would be more handy to deal with an array of (512 * 512 * 7). This 7-channel array gives a one-hot encoding of each pixel's category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 25\n",
    "cnt = 0\n",
    "for fig_id, input_img, bdry_img, mask_img, label in img_gen():\n",
    "    cnt += 1\n",
    "    if cnt == sample_idx:\n",
    "        print('Figure ID: %s' % fig_id)\n",
    "        plt.figure(0)\n",
    "        ax1 = plt.subplot(221)\n",
    "        ax1.imshow(input_img[:, :, :3])\n",
    "        ax2 = plt.subplot(222)\n",
    "        ax2.imshow(input_img[:, :, 3], cmap='gray')\n",
    "        ax3 = plt.subplot(223)\n",
    "        ax3.imshow(bdry_img, cmap='gray', vmin=0, vmax=1)\n",
    "        ax4 = plt.subplot(224)\n",
    "        ax4.imshow(mask_img, cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        plt.figure(1)\n",
    "        final_mask = np.multiply(bdry_img, mask_img) # The final mask is the intersection of the boundary and the mask.\n",
    "        masked_img = np.multiply(input_img, final_mask.reshape((512, 512, 1)))\n",
    "        ax1 = plt.subplot(131)\n",
    "        ax1.imshow(masked_img[:, :, :3])\n",
    "        ax2 = plt.subplot(132)\n",
    "        ax2.imshow(masked_img[:, :, 3], cmap='gray')\n",
    "        ax3 = plt.subplot(133)\n",
    "        ax3.imshow(final_mask, cmap='gray')\n",
    "        \n",
    "        plt.figure(2)\n",
    "        for i in range(6):\n",
    "            ax = plt.subplot(231 + i)\n",
    "            ax.imshow(label[:, :, i], cmap='gray', vmin=0, vmax=1)\n",
    "        \n",
    "        plt.figure(3)\n",
    "        plt.imshow(label[:, :, 6], cmap='gray', vmin=0, vmax=1)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the Boundary and the Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the number of positives for each class\n",
    "for fig_id, input_img, bdry_img, mask_img, label in img_gen():\n",
    "    for i in range(len(label))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of you are confused about how to handle the boundary and the mask. If you read the instruction from git github repo (https://github.com/SHI-Labs/Agriculture-Vision), then you would see that the mask and the boundary pretty much has the same function: pixels that appear to be black in either mask and or boundary are the pixels that you don't need to classify for! Therefore, what we would like to do is to multiply the mask with the boundary to form the \"final mask\", as we did in the code above. This \"final mask\" contains all information from the mask and the boundary.\n",
    "\n",
    "How to use the \"final mask\"? One option could be to multiply it with the RGB and NIR image. This \"blacks out\" all pixels that you don't need to consider in you classification. Although this is a convenient approach, it may gives you trouble when you start using more complicated methods like CNN, as the \"blacked out\" pixels are just zeros, and they still plays a roll in the whole network (affects the forward and backward process). Therefore, it is important to find a way to completely eliminate the influence of these pixels. This is a very important an interesting part of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and Train UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with UNET implementation\n",
    "\n",
    "def conv2d_block(input_tensor, n_filters, kernel_size = 3):\n",
    "    \"\"\"Function to add 2 convolutional layers with the parameters passed to it\"\"\"\n",
    "    # first layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "               kernel_initializer = 'he_normal', padding = 'same', activation = 'relu')(input_tensor)\n",
    "   \n",
    "    \n",
    "    # second layer\n",
    "    x = Conv2D(filters = n_filters, kernel_size = (kernel_size, kernel_size),\\\n",
    "              kernel_initializer = 'he_normal', padding = 'same', activation = 'relu')(x)\n",
    "    \n",
    "    return x\n",
    "def createUnet(input_img, n_filters = 16, dropout = 0.1):\n",
    "    #Down the Unet\n",
    "    conv1 = conv2d_block(input_img, n_filters)\n",
    "    pool1 = MaxPooling2D((2, 2))(conv1)\n",
    "    pool1 = Dropout(dropout)(pool1)\n",
    "    \n",
    "    conv2 = conv2d_block(pool1, n_filters*2)\n",
    "    pool2 = MaxPooling2D((2, 2))(conv2)\n",
    "    pool2 = Dropout(dropout)(pool2)\n",
    "    \n",
    "    conv3 = conv2d_block(pool2, n_filters*4)\n",
    "    pool3 = MaxPooling2D((2, 2))(conv3)\n",
    "    pool3 = Dropout(dropout)(pool3)\n",
    "    \n",
    "    conv4 = conv2d_block(pool3, n_filters*8)\n",
    "    pool4 = MaxPooling2D((2, 2))(conv4)\n",
    "    pool4 = Dropout(dropout)(pool4)\n",
    "    #bottom of the unet\n",
    "    conv5 = conv2d_block(pool4, n_filters*16)\n",
    "    \n",
    "    #up the U\n",
    "    uconv4 = Conv2DTranspose(n_filters * 8, (3, 3), strides = (2, 2), padding = 'same')(conv5)\n",
    "    uconv4 = concatenate([uconv4, conv4])\n",
    "    uconv4 = conv2d_block(uconv4, n_filters*8)\n",
    "    \n",
    "    uconv3 = Conv2DTranspose(n_filters * 4, (3, 3), strides = (2, 2), padding = 'same')(uconv4)\n",
    "    uconv3 = concatenate([uconv3, conv3])\n",
    "    uconv3 = conv2d_block(uconv3, n_filters*4)\n",
    "    \n",
    "    uconv2 = Conv2DTranspose(n_filters * 2, (3, 3), strides = (2, 2), padding = 'same')(uconv3)\n",
    "    uconv2 = concatenate([uconv2, conv2])\n",
    "    uconv2 = conv2d_block(uconv2, n_filters*2)\n",
    "    \n",
    "    uconv1 = Conv2DTranspose(n_filters * 1, (3, 3), strides = (2, 2), padding = 'same')(uconv2)\n",
    "    uconv1 = concatenate([uconv1, conv1])\n",
    "    uconv1= conv2d_block(uconv1, n_filters*1)\n",
    "    \n",
    "    output = Conv2D(filters = 7, kernel_size = (1,1), activation = \"softmax\", padding = 'same')(uconv1)\n",
    "    model = Model(inputs=[input_img], outputs=[output])\n",
    "    return model\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_img_gen(dataset='train'):\n",
    "    for fig_id, input_img, bdry_img, mask_img, label in img_gen(dataset):\n",
    "#     fig_id, input_img, bdry_img, mask_img, label =  next(img_gen(dataset))\n",
    "        final_mask = np.multiply(bdry_img, mask_img).reshape(512, 512, 1) # Form the final mask\n",
    "        masked_img = np.multiply(final_mask, input_img)\n",
    "    #     masked_img = tf.expand_dims(masked_img, axis =-1)\n",
    "        masked_img = masked_img.reshape(-1, 512, 512, 4)\n",
    "        if dataset != 'test':\n",
    "            label = label.reshape(-1, 512, 512, 7)\n",
    "            yield masked_img, label\n",
    "\n",
    "        else:\n",
    "            yield masked_img, fig_id\n",
    "# sample = next(img_gen('val'))\n",
    "# print(sample[1].shape, sample[4].shape)\n",
    "# print(next(masked_img_gen())[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from itertools import product\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "\n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "\n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    weights = K.variable(weights)\n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "        \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "input_img = Input((512, 512, 4), name='img')\n",
    "model = createUnet(input_img)\n",
    "weights = 10 * np.ones((7,))\n",
    "weights[0] = .3\n",
    "# weights = np.ones((7,7))\n",
    "# weights[:,6] = 1.3\n",
    "# weights[6,:] = 1.3\n",
    "model.compile(optimizer=\"adam\", loss=weighted_categorical_crossentropy(weights), metrics = [tf.keras.metrics.MeanIoU(num_classes=7)])\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AccHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.accs = []\n",
    "        self.acc_avg = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.accs.append(round(1e2*float(logs.get('acc')),4))\n",
    "        self.acc_avg.append(round(np.mean(self.accs,dtype=np.float64),4))\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = 12901//32\n",
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_steps = 4431//32\n",
    "print(validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 15\n",
    "model.fit(masked_img_gen('train'), epochs=epochs, steps_per_epoch=403, validation_data=masked_img_gen('val'), validation_steps=138, use_multiprocessing=False, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make predictions on the model\n",
    "X_list = []\n",
    "\n",
    "fig_ids = []\n",
    "cnt = 0\n",
    "for mask_img, fig_id in masked_img_gen('test'):\n",
    "    cnt += 1\n",
    "    fig_ids.append(fig_id)\n",
    "#     mask_img = np.delete(mask_img,obj = 1 axis = 0)\n",
    "#    X_list.append(mask_img)\n",
    "\n",
    "# print (len(X_list))\n",
    "# X_batch = np.stack(X_list, axis=0)\n",
    "\n",
    "# print(X_batch.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred= model.predict(masked_img_gen('test'), verbose =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyClassifier:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Randomly generate some parameters.\n",
    "        self.param = np.random.normal(size=(4, 7))\n",
    "        \n",
    "    def softmax(self, logits):\n",
    "        batch_size = logits.shape[0]\n",
    "        exp_val = np.exp(logits)\n",
    "        normalizer = np.sum(exp_val, axis=-1).reshape((batch_size, 512, 512, 1))\n",
    "        return exp_val / normalizer\n",
    "    \n",
    "    def predict(self, X):\n",
    "        result = self.softmax(X @ self.param)\n",
    "        return result\n",
    "    \n",
    "def MSE(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    For simplicity, we use the MSE loss, but it is better to use cross entropy loss with softmax.\n",
    "    \"\"\"\n",
    "    return np.sum(np.power(y_pred - y_true, 2)) / y_true.shape[0]\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "X_list = []\n",
    "y_list = []\n",
    "fig_ids = []\n",
    "for fig_id, input_img, bdry_img, mask_img, label in img_gen():\n",
    "    cnt += 1\n",
    "    fig_ids.append(fig_id)\n",
    "    y_list.append(label) # \n",
    "    final_mask = np.multiply(bdry_img, mask_img).reshape(512, 512, 1) # Form the final mask\n",
    "    masked_img = np.multiply(final_mask, input_img) # Multiply with the RGB and NIR image\n",
    "    X_list.append(masked_img)\n",
    "    if cnt == 10:\n",
    "        break\n",
    "X_batch = np.stack(X_list, axis=0)\n",
    "y_batch = np.stack(y_list, axis=0)\n",
    "# Check the shape of X and y.\n",
    "print(X_batch.shape, y_batch.shape)\n",
    "\n",
    "# Initialize a classifierModel was constructed with shape (None, 512, 512, 4) for input KerasTensor(type_spec=TensorSpec(shape=(None, 512, 512, 4), dtype=tf.float32, name='img'), name='img', description=\"created by layer 'img'\"), but it was called on an input with incompatible shape (None, None, None).\n",
    "classifier = DummyClassifier()\n",
    "# Predict on X_batch\n",
    "y_pred = classifier.predict(X_batch)\n",
    "print(y_pred.shape) # Expected to be of the same shape as y_batch\n",
    "# Compute the error\n",
    "error = MSE(y_batch, y_pred)\n",
    "print(error) # Expected to be very large since we are doing random classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Dummy Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following example, we take 10 samples from the training set, use a dummy classifier to train it, and compute loss with respect to the labels. The dummy classifier does nothing else but generate random classification, but this gives you a sense of what should be considered as input of the model, and what should be considered as output.\n",
    "\n",
    "In our example we will just use the very basic method to handle the mask and the boundary: we multiply the image with the \"final mask\". Keep in mind that you are expected to explore more method of handling the mask and the boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to save the 10 predictions under the name of their ID. Before saving the result, we first need to convert the 7-channel prediction (one-hot) into the 1-channel representation. Note that when generate the background label we appended as the 7th channel. Therefore, when converting to 1-channel representation, it would be handy to first bring that channel to the first channel, so that the channel index of each category matches the coresponding value indicator (0-6) of each category. The 10 predictions are saved under a directory named \"pred_example\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "if not os.path.isdir('preds'):\n",
    "    os.mkdir('preds')\n",
    "\n",
    "for mask_img, fig_id in masked_img_gen('test'):\n",
    "    \n",
    "    y_pred= model.predict(mask_img, verbose=0)\n",
    "    \n",
    "    pred_result = y_pred[0,:,:,:] # take the ith predictio\n",
    "    \n",
    "    \n",
    "    # Since we appended the background to the last channel, we need to bring it to the front when saving predictions\n",
    "    processed_result = np.concatenate([pred_result[:, :, -1].reshape((512, 512, 1)), pred_result[:, :, :-1]], axis=-1)\n",
    "#     print(processed_result.shape) # Should be (512, 512, 7)\n",
    "    # Convert the 7-channel result to 1-channel result and cast to uint8\n",
    "    \n",
    "    final_pred = np.argmax(processed_result, axis=-1).astype(np.uint8)\n",
    "#     print(final_pred.shape) # Should be (512, 512)\n",
    "    # Save the prediction\n",
    "    filename = os.path.join('preds', fig_id + '.png')\n",
    "    Image.fromarray(final_pred).save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! zip submission.zip preds/*.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zip the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, zip the files in the \"pred_example\" folder using:\\\n",
    "   ```zip submission.zip *.png```\\\n",
    "The result can be submitted to the server for evaluation (here we are predicting only 10 samples for the training set. For the server to process your submission, you need to predict all samples in the test set). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
